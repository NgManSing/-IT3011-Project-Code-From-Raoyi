{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         explan edit made usernam hardcor metallica fan...\n",
      "1         d'aww match background colour 'm seem stuck th...\n",
      "2         hey man 'm realli tri edit war 's guy constant...\n",
      "3         `` ca n't make real suggest improv wonder sect...\n",
      "4                             sir hero chanc rememb page 's\n",
      "                                ...                        \n",
      "159566    `` second time ask view complet contradict cov...\n",
      "159567       asham horribl thing put talk page 128.61.19.93\n",
      "159568    spitzer umm there actual articl prostitut ring...\n",
      "159569    look like actual put speedi first version dele...\n",
      "159570    `` ... realli n't think understand came idea b...\n",
      "Name: comment_text, Length: 159571, dtype: object\n",
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0           0             0        0       0       0              0\n",
      "1           0             0        0       0       0              0\n",
      "2           0             0        0       0       0              0\n",
      "3           0             0        0       0       0              0\n",
      "4           0             0        0       0       0              0\n",
      "...       ...           ...      ...     ...     ...            ...\n",
      "159566      0             0        0       0       0              0\n",
      "159567      0             0        0       0       0              0\n",
      "159568      0             0        0       0       0              0\n",
      "159569      0             0        0       0       0              0\n",
      "159570      0             0        0       0       0              0\n",
      "\n",
      "[159571 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# sent_list = train['comment_text'].values.tolist()\n",
    "# print(toxic_df[:10])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train=pd.read_csv(\"train2.csv\")\n",
    "\n",
    "\n",
    "# X = train.comment_text\n",
    "y = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "X = train.comment_text.apply(lambda x: np.str_(x))\n",
    "\n",
    "# train and validation set\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "vect = TfidfVectorizer(max_features=30000)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for different vectorizors and classifiers\n",
    "def test_vec_classifer(vect, clf):\n",
    "        \n",
    "    # create document-term matrices using the vectorizer\n",
    "\n",
    "    \n",
    "    # print the number of features that were generated\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    \n",
    "    # use Multinomial Naive Bayes to predict the star rating\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = clf.predict(X_test_dtm)\n",
    "    \n",
    "    # print the accuracy of its predictions\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "    #print('Confusion matrix: \\n', metrics.confusion_matrix(y_test, y_pred_class))\n",
    "    #y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "    #print('AUC: ', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.905121199207881\n"
     ]
    }
   ],
   "source": [
    "# using binary relevance, kernel killed (consume too much memory)\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "\n",
    "#classifier = BinaryRelevance(GaussianNB())\n",
    "classifier = BinaryRelevance(MultinomialNB())\n",
    " \n",
    "# train\n",
    "classifier.fit(X_train_dtm, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(X_test_dtm)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing toxic comments...**\n",
      "Test accuracy is 0.9569097335372121\n",
      "\n",
      "\n",
      "**Processing severe_toxic comments...**\n",
      "Test accuracy is 0.9903993181761211\n",
      "\n",
      "\n",
      "**Processing obscene comments...**\n",
      "Test accuracy is 0.9772140475772692\n",
      "\n",
      "\n",
      "**Processing threat comments...**\n",
      "Test accuracy is 0.9968916852580654\n",
      "\n",
      "\n",
      "**Processing insult comments...**\n",
      "Test accuracy is 0.9698443335923596\n",
      "\n",
      "\n",
      "**Processing identity_hate comments...**\n",
      "Test accuracy is 0.9919785426014589\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(X_train_dtm, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test_dtm)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9019376833028351\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kernel died\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(MultinomialNB())\n",
    "# train\n",
    "classifier.fit(X_train_dtm, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(X_test_dtm)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9099591407013762\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kernel died\n",
    "\n",
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(MultinomialNB())\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(X_train_dtm, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(X_test_dtm)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangraoyi/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_neighbors=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8974005464617852\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "classifier_new = MLkNN(n_neighbors=10)\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "x_train = lil_matrix(X_train_dtm).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(X_test_dtm).toarray()\n",
    "# train\n",
    "classifier_new.fit(X_train_dtm, y_train)\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(X_test_dtm)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
